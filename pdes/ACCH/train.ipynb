{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8ab614-0ec3-4f21-aae1-fc3e6e1767f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_ALLOCATOR=platform\n"
     ]
    }
   ],
   "source": [
    "# Environment variable\n",
    "%env XLA_PYTHON_CLIENT_ALLOCATOR=platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08941964-5f24-4f25-9b9d-0198b1026cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. Standard Python Libraries\n",
    "# ==============================================================================\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Third-Party Scientific & ML Libraries\n",
    "# ==============================================================================\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from jax import config, lax, random, vmap\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from jax.scipy.optimize import minimize\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. pinn_toolkit\n",
    "# ==============================================================================\n",
    "from pinn_toolkit.derivative import Derivative\n",
    "from pinn_toolkit.sampler import Sampler\n",
    "from pinn_toolkit.train import Train\n",
    "from pinn_toolkit.train_util import compute_err, generate_param\n",
    "from pinn_toolkit.util import (\n",
    "    L2, tree_to_f32, get_i, get_len, hex_to_key, key_to_hex,\n",
    "    load_model, save_model, load_pytree, save_pytree,\n",
    "    map_span, map_span_dict, split_pytree, stratified_subset,\n",
    "    load_h5,\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. local files\n",
    "# ==============================================================================\n",
    "from interactive_pde_suite import InteractivePDESuite\n",
    "from pde_dimless import PDE_dimless\n",
    "from residual import Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cbdde18-66ab-4ee9-9544-974483c91f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up PDE\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "pdeparams_phys = {\n",
    "    \"alpha_phi\": 9.62e-5, \"omega_phi\": 1.663e7, \"M\": 8.5e-10 / (2 * 5.35e7),\n",
    "    \"A\": 5.35e7, \"L\": 1e-11, \"c_se\": 1.0, \"c_le\": 5100/1.43e5, \"x_range\": (-50.0e-6, 50.0e-6),\n",
    "    \"t_range\": (0, 1.0e5), \"nx\": 32, \"nt\": 32, \"l_0\": 2*50.0e-6, \"t_0\": 1.0e5\n",
    "}\n",
    "\n",
    "pdedimless = PDE_dimless(pdeparams_phys)\n",
    "span_pde = {\n",
    "    'x':pdedimless.x_range_nd,\n",
    "    't':pdedimless.t_range_nd,\n",
    "    'L':(1e-12, 1e-10),\n",
    "    'M':(1e-21, 1e-19)\n",
    "}\n",
    "span_model = {\n",
    "    'x':(-0.5,0.5),\n",
    "    't':(0,1),\n",
    "    'L':(0,1),\n",
    "    'M':(0,1)\n",
    "}\n",
    "\n",
    "# load validation data\n",
    "validation_data_path = os.path.join('data', '2rqmc_5k_64*64') \n",
    "data_pde = load_h5(validation_data_path)\n",
    "ref_data = tree_to_f32(map_span_dict(data_pde, span_pde, span_model))\n",
    "\n",
    "# construct modelN\n",
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "#### set up the model\n",
    "from model import PINN\n",
    "inp_idx = {'x':0, 't':1, 'L':2, 'M':3}\n",
    "out_idx =  {'phi':0, 'c':1}\n",
    "# trunk_keys = ['x','t']\n",
    "# latent_size = 32\n",
    "# branch_size = [64,3]\n",
    "# trunk_size = [64,3]\n",
    "# transition_size = [32,2]\n",
    "base_model = PINN(inp_idx, out_idx, span_pde, span_model, width = 32, depth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f5fd730-db1a-4e7e-89bb-799c72c3eea3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model_single(key, pdeparams_phys, model, method, P_model, validation_chunk_size = 10, train_with_data = True):\n",
    "    # ==============================================================================\n",
    "    # --- 1. Preliminaries\n",
    "    # ==============================================================================\n",
    "    jax.config.update(\"jax_enable_x64\", True)\n",
    "    inp_idx = model.inp_idx\n",
    "    out_idx = model.out_idx\n",
    "    span_pde = model.span_pde\n",
    "    span_model = model.span_model\n",
    "    P_phys = map_span_dict(P_model, span_model, span_pde)\n",
    "\n",
    "    # --- generate training data\n",
    "    pdedimless = PDE_dimless(pdeparams_phys)\n",
    "    num_gt_to_use = pdeparams_phys['nx'] * pdeparams_phys['nt'] if train_with_data else 0\n",
    "    key, subkey = random.split(key)\n",
    "    _, train_data = pdedimless.generate_training_data(subkey, P_phys, num_gt_to_use)\n",
    "    train_data = map_span_dict(train_data, span_pde, span_model)\n",
    "    \n",
    "    # --- convert everything to fp32\n",
    "    jax.config.update(\"jax_enable_x64\", False)\n",
    "    P_model, P_phys, train_data, span_pde, span_model = map(\n",
    "        tree_to_f32,\n",
    "        (P_model, P_phys, train_data, span_pde, span_model)\n",
    "    )\n",
    "    \n",
    "    # --- config derivative\n",
    "    d = Derivative(inp_idx, out_idx, span_pde, span_model)\n",
    "    for deriv_name in ['phi_t', 'phi_x', 'phi_2x', 'c_t', 'c_2x']:\n",
    "        d.create_deriv_fn(deriv_name)\n",
    "\n",
    "    # --- construct residual\n",
    "    r = Residual(span_pde, span_model, pdedimless, d)\n",
    "    \n",
    "    # --- config sampling function\n",
    "    def input_single(key, P_i, train_data_i, num_data):\n",
    "        keys = random.split(key, 5)\n",
    "        inp = {k: {} for k in ['ic', 'bc', 'colloc', 'data']}\n",
    "        # ------ ic\n",
    "        inp['ic']['x'] = Sampler.get(keys[0], [16, 32, 16], [(-0.5, -0.1), (-0.1, 0.1), (0.1, 0.5)])\n",
    "        inp['ic']['t'] = Sampler.get(keys[0], [64], [(0, 0)])\n",
    "        # ------ bc\n",
    "        inp['bc']['x'] = Sampler.get(keys[1], [32, 32], [(-0.5, -0.5), (0.5, 0.5)])\n",
    "        inp['bc']['t'] = jnp.tile(Sampler.get(keys[1], [32], [(0, 1)]), 2)\n",
    "        # ------ colloc\n",
    "        x_colloc = Sampler.get(keys[2], [32], [(-0.5, 0.5)])\n",
    "        t_colloc = Sampler.get(keys[3], [32], [(0, 1)])\n",
    "        inp['colloc']['x'], inp['colloc']['t'] = map(lambda array: array.ravel(), jnp.meshgrid(x_colloc, t_colloc, indexing = \"ij\"))\n",
    "        # ------ data\n",
    "        inp['data'].update({key: train_data_i[key] for key in ['x','t','phi','c']})\n",
    "            \n",
    "        # Add parameters to inp\n",
    "        for inp_key in inp:\n",
    "            ref_array = inp[inp_key]['x']\n",
    "            for param_key, param_value in P_i.items():\n",
    "                inp[inp_key][param_key] = jnp.full(ref_array.shape, param_value, ref_array.dtype)\n",
    "                \n",
    "        return inp\n",
    "    \n",
    "    def new_input(key, P, train_data, num_data):\n",
    "        num_params = get_len(P)\n",
    "        keys = random.split(key, num_params)\n",
    "        out = jax.vmap(input_single, in_axes=(0, 0, 0, None))(keys, P, train_data, num_data)\n",
    "        return jax.tree_util.tree_map(lambda x: jnp.reshape(x, (-1,)), out)\n",
    "\n",
    "    def _subset(key, pytree, subsample_size):\n",
    "        return stratified_subset(key, pytree, get_len(pytree), get_len(P_model), subsample_size)\n",
    "    \n",
    "    def subset_input(key, inp, subset_size):\n",
    "        num_subsets = len(subset_size)\n",
    "        subkeys = random.split(key, num_subsets)\n",
    "        return {k: _subset(subkey, inp[k], size) for (k, size), subkey in zip(subset_size.items(), subkeys)}\n",
    "\n",
    "    # ==============================================================================\n",
    "    # --- Create wrapper functions for training\n",
    "    # ==============================================================================\n",
    "    def update_input(key, P, train_data, num_data):\n",
    "        return new_input(key, P, train_data, num_data)\n",
    "\n",
    "    def update_weight(key, params, static, inp):\n",
    "        # ------ compute NTK weight\n",
    "        model_temp = eqx.combine(params, static)\n",
    "        ntk_size = {'ic': 4, 'bc': 4, 'colloc': 8, 'data': 8}\n",
    "        subset_to_use = subset_input(key, inp, ntk_size)\n",
    "        ntk_weight = r.compute_ntk_weights(model_temp, subset_to_use)\n",
    "        \n",
    "        # ------ apply manual weight\n",
    "        manual_weight = {'ic': 1.0, 'bc': 1.0, 'ac': 1.0, 'ch': 1.0, 'data': 1.0}\n",
    "        weighted = {k: ntk_weight[k] * manual_weight[k] for k in ntk_weight}\n",
    "        \n",
    "        # ------ noramlize final weight\n",
    "        weights_array = jnp.array(list(weighted.values()))\n",
    "        geom_mean = jnp.exp(jnp.mean(jnp.log(weights_array + 1e-12)))  # add epsilon to avoid log(0)\n",
    "        normalized = {k: v / geom_mean for k, v in weighted.items()}\n",
    "    \n",
    "        return normalized\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def loss_fn(params, static, inp, weight_dict):\n",
    "        model_temp = eqx.combine(params, static)\n",
    "        loss_dict = r.compute_loss(model_temp, inp)\n",
    "        weighted_loss = jnp.sum(jnp.array([weight_dict[k] * loss_dict[k] for k in ['ic', 'bc', 'ac', 'ch', 'data']]))\n",
    "        return weighted_loss, loss_dict\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def validation_fn(params, static):\n",
    "        return compute_err(params, static, ref_data, validation_chunk_size)\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # --- Initialize Training Configurations\n",
    "    # ==============================================================================\n",
    "    key, subkey = random.split(key)\n",
    "    total_steps = 60000\n",
    "    sp1 = 200  # Resampling frequency\n",
    "    sp2 = 500   # NTK weighting frequency\n",
    "    sp3 = 30000 # log frequency (set very high to prevent logging)\n",
    "    sp4 = 100    # Validation frequency\n",
    "    log_keys_order = ('ic', 'bc', 'ac', 'ch', 'data', 'L2_phi', 'L2_c') # Use a tuple\n",
    "    \n",
    "    # --- generate new input\n",
    "    key, subkey = random.split(key)\n",
    "    inp = new_input(subkey, P_model, train_data, num_gt_to_use)\n",
    "    \n",
    "    # --- model param and static\n",
    "    model_params, model_static = eqx.partition(model, eqx.is_inexact_array)\n",
    "    \n",
    "    # --- optimizer\n",
    "    initial_lr = 1e-3\n",
    "    # Use a learning rate schedule (e.g., exponential decay)\n",
    "    lr_schedule = optax.exponential_decay(\n",
    "        init_value=initial_lr,\n",
    "        transition_steps=total_steps,\n",
    "        decay_rate=0.5 # This means LR will be (decay_rate*100)% of initial lr at the end\n",
    "    )\n",
    "    \n",
    "    max_grad_norm = 1.0\n",
    "    weight_decay_adamw = 1e-2\n",
    "    \n",
    "    optimizer_adamw = optax.chain(\n",
    "        optax.clip_by_global_norm(max_grad_norm),\n",
    "        optax.adamw(\n",
    "            learning_rate=lr_schedule, # <<< USE THE SCHEDULE HERE\n",
    "            b1=0.9,\n",
    "            b2=0.999,\n",
    "            weight_decay=weight_decay_adamw\n",
    "        )\n",
    "    )\n",
    "    opt_state = optimizer_adamw.init(model_params)\n",
    "    \n",
    "    # --- initialize carry\n",
    "    key, subkey = random.split(key)\n",
    "    weight_dict = update_weight(key, model_params, model_static, inp)\n",
    "    init_carry = (subkey, inp, weight_dict, model_params, opt_state, jnp.inf, model_params)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # --- Train the model\n",
    "    # ==============================================================================\n",
    "    final_carry, loss_history = Train.train(\n",
    "        total_steps=total_steps,\n",
    "        sp1=sp1,\n",
    "        sp2=sp2,\n",
    "        sp3=sp3,\n",
    "        sp4=sp4,\n",
    "        num_data=num_gt_to_use,\n",
    "        static=model_static,\n",
    "        optimizer=optimizer_adamw,\n",
    "        update_input=update_input,\n",
    "        update_weight=update_weight,\n",
    "        loss_fn=loss_fn,\n",
    "        validation_fn = validation_fn,\n",
    "        log_keys_order=log_keys_order,\n",
    "        P_model=P_model,\n",
    "        train_data=train_data,\n",
    "        carry=init_carry\n",
    "    )\n",
    "    \n",
    "    # --- reconstruction\n",
    "    _, _, _, final_param, _, best_geom, param_geom = final_carry\n",
    "    model_geom = eqx.combine(param_geom, model_static)\n",
    "    print(f\"the best loss is {best_geom}\")\n",
    "    return model_geom, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58803d0b-7198-4e1d-9d11-f1978ef90607",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model_batch(key, pdeparams_phys, base_model, method, size, note=\"\", repetition=1,\n",
    "                      param_key_override = None, out_dir = \"models\", validation_chunk_size = 10):\n",
    "    \"\"\"\n",
    "    Trains models in batches, saving each model and its loss history into a dedicated subdirectory.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(repetition):\n",
    "        print(f\"================= Batch repetition {i+1} / {repetition} =================\")\n",
    "        key, subkey = random.split(key)\n",
    "        param_key, train_key = random.split(subkey,2)\n",
    "        prama_key = param_key_override if param_key_override else param_key # override the param_key if necessary\n",
    "        param_key_hex, train_key_hex = key_to_hex(param_key), key_to_hex(train_key)\n",
    "        \n",
    "        # 1. create the directory and file names\n",
    "        sub_dir = f\"{note}_{method}_{size}\" if note else f\"{method}_{size}\"\n",
    "        subsub_dir = f\"{param_key_hex}_{train_key_hex}\"\n",
    "        final_dir = os.path.join(out_dir, sub_dir, subsub_dir)\n",
    "        model_filename = f\"model_{sub_dir}_{subsub_dir}.pkl\"\n",
    "        loss_filename = f\"loss_{sub_dir}_{subsub_dir}.npz\"\n",
    "        model_path = os.path.join(final_dir, model_filename)\n",
    "        loss_path = os.path.join(final_dir, loss_filename)\n",
    "        \n",
    "        # 2. if model exists\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Model {note}_{method}_{size}_{subsub_dir} exists, skipping to next model.\")\n",
    "            continue\n",
    "\n",
    "        # 3. if model does not exist\n",
    "        # --- create the directory to store model\n",
    "        os.makedirs(final_dir, exist_ok=True)\n",
    "        log_info_with_note = f\"Repetition {i}: Start training | method={note}_{method} | size={size} | key={subsub_dir} | \"\n",
    "        log_info_without_note = f\"Repetition {i}: Start training | method={method} | size={size} | key={subsub_dir} | \"\n",
    "        print(log_info_with_note) if note else print(log_info_without_note)\n",
    "\n",
    "        # track the time consumed\n",
    "        ts = time.time()\n",
    "        # --- begin training\n",
    "        \n",
    "        P_model = generate_param(param_key, method, size, span_model)\n",
    "        trained_model, loss_history = train_model_single(train_key, pdeparams_phys, base_model, method, P_model, validation_chunk_size)\n",
    "        # --- training finished, lkog and save time.\n",
    "        print(f\"Total time elapsed : {(time.time()-ts)/60:.2f} minutes\")\n",
    "        save_model(trained_model, model_path)\n",
    "        save_pytree(loss_history, loss_path)\n",
    "        print(f\"Saved model and loss to: {final_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae7c974-a467-4633-acbe-db3298dc6445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5834283828735352\n"
     ]
    }
   ],
   "source": [
    "# test if compute_err is working properly\n",
    "chunk_size = 10\n",
    "params, static = eqx.partition(base_model, eqx.is_inexact_array)\n",
    "ts = time.time()\n",
    "compute_err(params, static, ref_data, chunk_size)\n",
    "print(time.time()-ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e077e730-db18-4ad9-ae56-0dd400c0fc52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Batch repetition 1 / 3 =================\n",
      "Model test_run_sobol_10_3eb9a7683ee2ea47_84a2fb5a191fbc74 exists, skipping to next model.\n",
      "================= Batch repetition 2 / 3 =================\n",
      "Model test_run_sobol_10_ab9434a46dd342d5_a20a34a02ec9faa6 exists, skipping to next model.\n",
      "================= Batch repetition 3 / 3 =================\n",
      "Model test_run_sobol_10_3f4628a3192bab30_163c93eb2e9d272a exists, skipping to next model.\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(18696422)\n",
    "out_dir = \"models\"\n",
    "train_model_batch(key, pdeparams_phys, base_model, 'sobol', 10, note = \"test_run\", repetition = 3, out_dir = out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9252c2-5490-408f-b041-bfb7f930635b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Batch repetition 1 / 3 =================\n",
      "Model test_run_uniformRandom_10_79eb06323ba8667a_47c6367f83f30347 exists, skipping to next model.\n",
      "================= Batch repetition 2 / 3 =================\n",
      "Model test_run_uniformRandom_10_27142e2de57df7a8_c0f8aac4dc8fd0b5 exists, skipping to next model.\n",
      "================= Batch repetition 3 / 3 =================\n",
      "Repetition 2: Start training | method=test_run_uniformRandom | size=10 | key=990807eb8dfd677c_98d5639024d22581 | \n",
      "=====================================================================================\n",
      "Step 0      total_loss = 14.9755     epoch_elapsed = 20.697s  total_elapsed = 20.697s\n",
      "Keys   :      ic      |      bc      |      ac      |      ch      |     data     |    L2_phi    |     L2_c    \n",
      "Weights: 1.3194e+01   | 1.2220e+01   | 2.1585e-04   | 2.3228e+00   | 1.2370e+01   | 1.0000e+00   | 1.0000e+00  \n",
      "Losses : 4.0205e-01   | 4.3537e-01   | 1.8686e+02   | 2.6730e-02   | 2.6472e-01   | 5.0043e-01   | 4.7301e-01  \n",
      "W * L  : 5.3047e+00   | 5.3204e+00   | 4.0333e-02   | 6.2088e-02   | 3.2746e+00   | 5.0043e-01   | 4.7301e-01  \n",
      "=======================================================================================\n",
      "Step 30000  total_loss = 0.2017      epoch_elapsed = 464.727s  total_elapsed = 485.424s\n",
      "Keys   :      ic      |      bc      |      ac      |      ch      |     data     |    L2_phi    |     L2_c    \n",
      "Weights: 7.5259e+01   | 1.1632e+03   | 2.1215e-02   | 3.1552e-06   | 1.7065e+02   | 1.0000e+00   | 1.0000e+00  \n",
      "Losses : 1.3069e-04   | 1.7832e-06   | 2.8660e-01   | 8.0459e+02   | 2.8912e-04   | 5.9403e-02   | 7.2389e-02  \n",
      "W * L  : 9.8358e-03   | 2.0742e-03   | 6.0802e-03   | 2.5387e-03   | 4.9339e-02   | 5.9403e-02   | 7.2389e-02  \n",
      "=======================================================================================\n",
      "Step 60000  total_loss = 0.1674      epoch_elapsed = 465.022s  total_elapsed = 950.446s\n",
      "Keys   :      ic      |      bc      |      ac      |      ch      |     data     |    L2_phi    |     L2_c    \n",
      "Weights: 6.5736e+01   | 1.3169e+03   | 2.6701e-02   | 3.0040e-06   | 1.4402e+02   | 1.0000e+00   | 1.0000e+00  \n",
      "Losses : 7.3532e-05   | 8.9785e-07   | 1.4721e-01   | 4.7973e+02   | 1.8578e-04   | 5.8095e-02   | 7.1113e-02  \n",
      "W * L  : 4.8337e-03   | 1.1824e-03   | 3.9307e-03   | 1.4411e-03   | 2.6756e-02   | 5.8095e-02   | 7.1113e-02  \n",
      "The training time is 15.84 minutes\n",
      "the best loss is 0.0033393241465091705\n",
      "Total time elapsed : 16.20 minutes\n",
      "Saved model and loss to: models/test_run_uniformRandom_10/990807eb8dfd677c_98d5639024d22581\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(44243546190052)\n",
    "out_dir = \"models\"\n",
    "train_model_batch(key, pdeparams_phys, base_model, 'uniformRandom', 10, note = \"test_run\", repetition = 3, out_dir = out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9441d-9a4e-4798-b4cd-7df08db30c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= Batch repetition 1 / 3 =================\n",
      "Repetition 0: Start training | method=test_run_grid | size=10 | key=d3e068992676a75b_17045d52f73300ee | \n",
      "=====================================================================================\n",
      "Step 0      total_loss = 15.1060     epoch_elapsed = 15.422s  total_elapsed = 15.422s\n",
      "Keys   :      ic      |      bc      |      ac      |      ch      |     data     |    L2_phi    |     L2_c    \n",
      "Weights: 1.3134e+01   | 1.2171e+01   | 2.4300e-04   | 2.0899e+00   | 1.2318e+01   | 1.0000e+00   | 1.0000e+00  \n",
      "Losses : 4.0317e-01   | 4.3639e-01   | 1.8493e+02   | 2.7504e-02   | 2.7793e-01   | 5.0043e-01   | 4.7299e-01  \n",
      "W * L  : 5.2952e+00   | 5.3115e+00   | 4.4938e-02   | 5.7481e-02   | 3.4234e+00   | 5.0043e-01   | 4.7299e-01  \n",
      "=======================================================================================\n",
      "Step 30000  total_loss = 0.1832      epoch_elapsed = 476.516s  total_elapsed = 491.938s\n",
      "Keys   :      ic      |      bc      |      ac      |      ch      |     data     |    L2_phi    |     L2_c    \n",
      "Weights: 9.5556e+01   | 9.8884e+02   | 2.0712e-02   | 2.2651e-06   | 2.2559e+02   | 1.0000e+00   | 1.0000e+00  \n",
      "Losses : 1.2961e-04   | 2.4276e-06   | 3.5403e-01   | 8.4201e+02   | 2.1345e-04   | 4.9863e-02   | 6.1209e-02  \n",
      "W * L  : 1.2385e-02   | 2.4005e-03   | 7.3325e-03   | 1.9072e-03   | 4.8152e-02   | 4.9863e-02   | 6.1209e-02  \n",
      "=======================================================================================\n",
      "Step 60000  total_loss = 0.1644      epoch_elapsed = 472.328s  total_elapsed = 964.266s\n",
      "Keys   :      ic      |      bc      |      ac      |      ch      |     data     |    L2_phi    |     L2_c    \n",
      "Weights: 9.0565e+01   | 1.1620e+03   | 2.5107e-02   | 2.4043e-06   | 1.5742e+02   | 1.0000e+00   | 1.0000e+00  \n",
      "Losses : 7.2698e-05   | 1.2261e-06   | 1.5615e-01   | 5.9369e+02   | 1.3946e-04   | 5.8017e-02   | 7.1034e-02  \n",
      "W * L  : 6.5839e-03   | 1.4247e-03   | 3.9205e-03   | 1.4274e-03   | 2.1953e-02   | 5.8017e-02   | 7.1034e-02  \n",
      "The training time is 16.07 minutes\n",
      "the best loss is 0.0031465899664908648\n",
      "Total time elapsed : 16.31 minutes\n",
      "Saved model and loss to: models/test_run_grid_10/d3e068992676a75b_17045d52f73300ee\n",
      "================= Batch repetition 2 / 3 =================\n",
      "Repetition 1: Start training | method=test_run_grid | size=10 | key=521a771f22f6a741_6ce1dadcbef36305 | \n",
      "=====================================================================================\n",
      "Step 0      total_loss = 15.0718     epoch_elapsed = 15.526s  total_elapsed = 15.526s\n",
      "Keys   :      ic      |      bc      |      ac      |      ch      |     data     |    L2_phi    |     L2_c    \n",
      "Weights: 1.3105e+01   | 1.2126e+01   | 2.4343e-04   | 2.1030e+00   | 1.2292e+01   | 1.0000e+00   | 1.0000e+00  \n",
      "Losses : 4.0355e-01   | 4.3636e-01   | 1.8481e+02   | 2.7292e-02   | 2.7793e-01   | 5.0043e-01   | 4.7299e-01  \n",
      "W * L  : 5.2885e+00   | 5.2912e+00   | 4.4990e-02   | 5.7394e-02   | 3.4163e+00   | 5.0043e-01   | 4.7299e-01  \n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(612565226762)\n",
    "out_dir = \"models\"\n",
    "train_model_batch(key, pdeparams_phys, base_model, 'grid', 10, note = \"test_run\", repetition = 3, out_dir = out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fc245-b1a8-4e17-a859-ad857696b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(3463523192)\n",
    "out_dir = \"models\"\n",
    "train_model_batch(key, pdeparams_phys, base_model, 'gridInner', 10, note = \"test_run\", repetition = 3, out_dir = out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ba60f-7918-42e4-90e9-8e12e6e57cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'models/test_run_sobol_5/366b71021d1e5b49_f5653be6ce868810/model_test_run_sobol_5_366b71021d1e5b49_f5653be6ce868810.pkl'\n",
    "# model_test = load_model(base_model, model_path)\n",
    "# model_test_params, model_test_static = eqx.partition(model_test, eqx.is_inexact_array)\n",
    "# compute_err(model_test_params, model_test_static, ref_data, 5)\n",
    "\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "# pdeparams_phys = {\n",
    "#     \"alpha_phi\": 9.62e-5, \"omega_phi\": 1.663e7, \"M\": 8.5e-10 / (2 * 5.35e7),\n",
    "#     \"A\": 5.35e7, \"L\": 1e-11, \"c_se\": 1.0, \"c_le\": 5100/1.43e5, \"x_range\": (-50.0e-6, 50.0e-6),\n",
    "#     \"t_range\": (0, 1.0e5), \"nx\": 128, \"nt\": 64, \"l_0\": 2*50.0e-6, \"t_0\": 1.0e5\n",
    "# }\n",
    "\n",
    "# pdedimless = PDE_dimless(pdeparams_phys)\n",
    "# num_gt_to_use = pdeparams_phys['nx'] * pdeparams_phys['nt']\n",
    "# span_pde = {'x':pdedimless.x_range_nd,'t':pdedimless.t_range_nd,'L':(1e-12, 1e-10),'M':(1e-21, 1e-19)}\n",
    "# span_model = {'x':(-0.5,0.5), 't':(0,1),'L':(0,1),'M':(0,1)}\n",
    "    \n",
    "# suite = InteractivePDESuite(pdeparams_phys)\n",
    "# suite.create_interactive_comparison_plot(\n",
    "#     model= model_test,\n",
    "#     span_pde=span_pde,\n",
    "#     span_model=span_model,\n",
    "#     num_frames=10,\n",
    "#     prediction_color = 'red'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273db766-211b-4ea6-bb5b-61cdb8e325d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_NTK import *\n",
    "# model = model_test\n",
    "# train_with_data = True\n",
    "# P_model = generate_param(hex_to_key('6f1dce3fa814fc8b'),'sobol', 5, span_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0831f-28c3-4d17-8162-9695e0f5cfa7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "# inp_idx = model.inp_idx\n",
    "# out_idx = model.out_idx\n",
    "# span_pde = model.span_pde\n",
    "# span_model = model.span_model\n",
    "# P_phys = map_span_dict(P_model, span_model, span_pde)\n",
    "\n",
    "# # --- generate training data\n",
    "# pdedimless = PDE_dimless(pdeparams_phys)\n",
    "# num_gt_to_use = pdeparams_phys['nx'] * pdeparams_phys['nt'] if train_with_data else 0\n",
    "# key, subkey = random.split(key)\n",
    "# _, train_data = pdedimless.generate_training_data(subkey, P_phys, num_gt_to_use)\n",
    "# train_data = map_span_dict(train_data, span_pde, span_model)\n",
    "\n",
    "# # --- convert everything to fp32\n",
    "# jax.config.update(\"jax_enable_x64\", False)\n",
    "# P_model, P_phys, train_data, span_pde, span_model = map(\n",
    "#     tree_to_f32,\n",
    "#     (P_model, P_phys, train_data, span_pde, span_model)\n",
    "# )\n",
    "\n",
    "# # --- config derivative\n",
    "# d = Derivative(inp_idx, out_idx, span_pde, span_model)\n",
    "# for deriv_name in ['phi_t', 'phi_x', 'phi_2x', 'c_t', 'c_2x']:\n",
    "#     d.create_deriv_fn(deriv_name)\n",
    "\n",
    "# # --- construct residual\n",
    "# r = Residual(span_pde, span_model, pdedimless, d)\n",
    "\n",
    "# # --- config sampling function\n",
    "# def input_single(key, P_i, train_data_i, num_data):\n",
    "#     keys = random.split(key, 5)\n",
    "#     inp = {k: {} for k in ['ic', 'bc', 'colloc', 'data']}\n",
    "#     # ------ ic\n",
    "#     inp['ic']['x'] = Sampler.get(keys[0], [16, 32, 16], [(-0.5, -0.1), (-0.1, 0.1), (0.1, 0.5)])\n",
    "#     inp['ic']['t'] = Sampler.get(keys[0], [64], [(0, 0)])\n",
    "#     # ------ bc\n",
    "#     inp['bc']['x'] = Sampler.get(keys[1], [32, 32], [(-0.5, -0.5), (0.5, 0.5)])\n",
    "#     inp['bc']['t'] = jnp.tile(Sampler.get(keys[1], [32], [(0, 1)]), 2)\n",
    "#     # ------ colloc\n",
    "#     x_colloc = Sampler.get(keys[2], [16], [(-0.5, 0.5)])\n",
    "#     t_colloc = Sampler.get(keys[3], [16], [(0, 1)])\n",
    "#     inp['colloc']['x'], inp['colloc']['t'] = map(lambda array: array.ravel(), jnp.meshgrid(x_colloc, t_colloc, indexing = \"ij\"))\n",
    "#     # ------ data\n",
    "#     inp['data'].update({key: train_data_i[key] for key in ['x','t','phi','c']})\n",
    "        \n",
    "#     # Add parameters to inp\n",
    "#     for inp_key in inp:\n",
    "#         ref_array = inp[inp_key]['x']\n",
    "#         for param_key, param_value in P_i.items():\n",
    "#             inp[inp_key][param_key] = jnp.full(ref_array.shape, param_value, ref_array.dtype)\n",
    "            \n",
    "#     return inp\n",
    "\n",
    "# def new_input(key, P, train_data, num_data):\n",
    "#     num_params = get_len(P)\n",
    "#     keys = random.split(key, num_params)\n",
    "#     out = jax.vmap(input_single, in_axes=(0, 0, 0, None))(keys, P, train_data, num_data)\n",
    "#     return jax.tree_util.tree_map(lambda x: jnp.reshape(x, (-1,)), out)\n",
    "\n",
    "# def _subset(key, pytree, subsample_size):\n",
    "#     return stratified_subset(key, pytree, get_len(pytree), get_len(P_model), subsample_size)\n",
    "\n",
    "# # --- Helper function for sorting (place this before subset_input) ---\n",
    "# def sort_points_dict(points_dict):\n",
    "#     \"\"\"\n",
    "#     Sorts a dictionary of arrays based on a predefined key order: L, M, t, x.\n",
    "#     \"\"\"\n",
    "#     # Define the desired sort order. This is the order of precedence.\n",
    "#     sort_order = ['L', 'M', 't', 'x']\n",
    "\n",
    "#     # Check if all necessary keys for sorting are present.\n",
    "#     if not all(key in points_dict for key in sort_order):\n",
    "#         # This is expected for the 'data' component if it doesn't have L, M.\n",
    "#         # We return it unsorted as there's no basis for sorting.\n",
    "#         return points_dict\n",
    "\n",
    "#     # jnp.lexsort sorts by the LAST key in the tuple first (primary key).\n",
    "#     # So, we provide the keys in reverse order of precedence.\n",
    "#     lexsort_keys = tuple(points_dict[key] for key in reversed(sort_order))\n",
    "#     sort_indices = jnp.lexsort(lexsort_keys)\n",
    "\n",
    "#     # Apply these same indices to every array in the dictionary.\n",
    "#     return tree_util.tree_map(lambda leaf: leaf[sort_indices], points_dict)\n",
    "\n",
    "\n",
    "# # --- Your NEW, INTEGRATED subset_input function ---\n",
    "# def subset_input(key, inp, subset_size, P_model):\n",
    "#     \"\"\"\n",
    "#     Performs stratified subsampling and then re-sorts the results to ensure\n",
    "#     a canonical order based on (L, M, t, x) for correct NTK plotting.\n",
    "\n",
    "#     Args:\n",
    "#         key (jax.random.PRNGKey): JAX random key.\n",
    "#         inp (dict): The full input dictionary from new_input.\n",
    "#         subset_size (dict): A dictionary specifying the number of points to sample\n",
    "#                             for each component (e.g., {'ic': 16, 'colloc': 64}).\n",
    "#         P_model (pytree): The pytree of model parameters (L, M), used to determine\n",
    "#                           the number of strata for sampling.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: The subsampled and correctly sorted input dictionary.\n",
    "#     \"\"\"\n",
    "#     # Your existing helper function for stratified subsetting\n",
    "#     def _subset(k, pytree, n_samples):\n",
    "#         # Note: get_len is assumed to be defined in your environment\n",
    "#         num_strata = get_len(P_model)\n",
    "#         total_size = get_len(pytree)\n",
    "#         return stratified_subset(k, pytree, total_size, num_strata, n_samples)\n",
    "\n",
    "#     num_subsets = len(subset_size)\n",
    "#     subkeys = random.split(key, num_subsets)\n",
    "\n",
    "#     # 1. Perform the stratified subsampling (this shuffles the data)\n",
    "#     subsampled_inp = {k: _subset(subkey, inp[k], size)\n",
    "#                       for (k, size), subkey in zip(subset_size.items(), subkeys)}\n",
    "\n",
    "#     # 2. Re-sort each component to restore the canonical parameter order\n",
    "#     print(\"Re-sorting subsampled inputs to ensure correct parameter ordering for plotting...\")\n",
    "#     sorted_inp = {k: sort_points_dict(v) for k, v in subsampled_inp.items()}\n",
    "\n",
    "#     return sorted_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9677b14-2abe-4701-95e8-b52e9c7ae344",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# key = jax.random.PRNGKey(1314159261375)\n",
    "# key, subkey = random.split(key)\n",
    "# inp = new_input(key, P_model, train_data, num_gt_to_use)\n",
    "# ntk_size = {'ic': 32, 'bc': 32, 'colloc': 128, 'data': 16*4}\n",
    "#inp = subset_input(subkey, inp, ntk_size, P_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ad3b6-6332-42ba-9011-d375d90cd5d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# NTK = get_NTK(\n",
    "#     model_test, inp, r, P_model,\n",
    "#     batch_size = 128, use_symlog = False,\n",
    "#     plot_subgrids = \"plot\", out_dir = \"plots\", plot_name = \"NTK_data.png\",\n",
    "#     dpi = 1200, selected_blocks = [\"data\"], ram_limit_gb = 8.0\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
